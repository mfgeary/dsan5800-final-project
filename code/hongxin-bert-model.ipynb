{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a59c5817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aea9b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bd40540",
   "metadata": {},
   "outputs": [],
   "source": [
    "homophones = {\n",
    "    \"accept\": [\"except\"],\n",
    "    \"except\": [\"accept\"],\n",
    "    \"affect\": [\"effect\"],\n",
    "    \"effect\": [\"affect\"],\n",
    "    \"allude\": [\"elude\"],\n",
    "    \"elude\": [\"allude\"],\n",
    "    \"altar\": [\"alter\"],\n",
    "    \"alter\": [\"altar\"],\n",
    "    \"arc\": [\"ark\"],\n",
    "    \"ark\": [\"arc\"],\n",
    "    \"bare\": [\"bear\"],\n",
    "    \"bear\": [\"bare\"],\n",
    "    \"beach\": [\"beech\"],\n",
    "    \"beech\": [\"beach\"],\n",
    "    \"berry\": [\"bury\"],\n",
    "    \"bury\": [\"berry\"],\n",
    "    \"billed\": [\"build\"],\n",
    "    \"build\": [\"billed\"],\n",
    "    \"blue\": [\"blew\"],\n",
    "    \"blew\": [\"blue\"],\n",
    "    \"board\": [\"bored\"],\n",
    "    \"bored\": [\"board\"],\n",
    "    \"brake\": [\"break\"],\n",
    "    \"break\": [\"brake\"],\n",
    "    \"buy\": [\"by\", \"bye\"],\n",
    "    \"by\": [\"buy\", \"bye\"],\n",
    "    \"bye\": [\"buy\", \"by\"],\n",
    "    \"cell\": [\"sell\"],\n",
    "    \"sell\": [\"cell\"],\n",
    "    \"cent\": [\"scent\", \"sent\"],\n",
    "    \"scent\": [\"cent\", \"sent\"],\n",
    "    \"sent\": [\"cent\", \"scent\"],\n",
    "    \"cite\": [\"site\", \"sight\"],\n",
    "    \"site\": [\"cite\", \"sight\"],\n",
    "    \"sight\": [\"cite\", \"site\"],\n",
    "    \"complement\": [\"compliment\"],\n",
    "    \"compliment\": [\"complement\"],\n",
    "    \"coarse\": [\"course\"],\n",
    "    \"course\": [\"coarse\"],\n",
    "    \"dear\": [\"deer\"],\n",
    "    \"deer\": [\"dear\"],\n",
    "    \"die\": [\"dye\"],\n",
    "    \"dye\": [\"die\"],\n",
    "    \"fair\": [\"fare\"],\n",
    "    \"fare\": [\"fair\"],\n",
    "    \"flour\": [\"flower\"],\n",
    "    \"flower\": [\"flour\"],\n",
    "    \"for\": [\"four\"],\n",
    "    \"four\": [\"for\"],\n",
    "    \"hair\": [\"hare\"],\n",
    "    \"hare\": [\"hair\"],\n",
    "    \"heal\": [\"heel\", \"he'll\"],\n",
    "    \"heel\": [\"heal\", \"he'll\"],\n",
    "    \"he'll\": [\"heal\", \"heel\"],\n",
    "    \"here\": [\"hear\"],\n",
    "    \"hear\": [\"here\"],\n",
    "    \"higher\": [\"hire\"],\n",
    "    \"hire\": [\"higher\"],\n",
    "    \"hole\": [\"whole\"],\n",
    "    \"whole\": [\"hole\"],\n",
    "    \"hour\": [\"our\"],\n",
    "    \"our\": [\"hour\"],\n",
    "    \"knight\": [\"night\"],\n",
    "    \"night\": [\"knight\"],\n",
    "    \"knot\": [\"not\"],\n",
    "    \"not\": [\"knot\"],\n",
    "    \"know\": [\"no\"],\n",
    "    \"no\": [\"know\"],\n",
    "    \"made\": [\"maid\"],\n",
    "    \"maid\": [\"made\"],\n",
    "    \"mail\": [\"male\"],\n",
    "    \"male\": [\"mail\"],\n",
    "    \"meat\": [\"meet\", \"mete\"],\n",
    "    \"meet\": [\"meat\", \"mete\"],\n",
    "    \"mete\": [\"meat\", \"meet\"],\n",
    "    \"morning\": [\"mourning\"],\n",
    "    \"mourning\": [\"morning\"],\n",
    "    \"new\": [\"knew\"],\n",
    "    \"knew\": [\"new\"],\n",
    "    \"none\": [\"nun\"],\n",
    "    \"nun\": [\"none\"],\n",
    "    \"one\": [\"won\"],\n",
    "    \"won\": [\"one\"],\n",
    "    \"pair\": [\"pare\", \"pear\"],\n",
    "    \"pare\": [\"pair\", \"pear\"],\n",
    "    \"pear\": [\"pair\", \"pare\"],\n",
    "    \"peace\": [\"piece\"],\n",
    "    \"piece\": [\"peace\"],\n",
    "    \"plain\": [\"plane\"],\n",
    "    \"plane\": [\"plain\"],\n",
    "    \"principal\": [\"principle\"],\n",
    "    \"principle\": [\"principal\"],\n",
    "    \"rain\": [\"reign\", \"rein\"],\n",
    "    \"reign\": [\"rain\", \"rein\"],\n",
    "    \"rein\": [\"rain\", \"reign\"],\n",
    "    \"right\": [\"rite\", \"write\"],\n",
    "    \"rite\": [\"right\", \"write\"],\n",
    "    \"write\": [\"right\", \"rite\"],\n",
    "    \"sea\": [\"see\"],\n",
    "    \"see\": [\"sea\"],\n",
    "    \"sew\": [\"so\", \"sow\"],\n",
    "    \"so\": [\"sew\", \"sow\"],\n",
    "    \"sow\": [\"sew\", \"so\"],\n",
    "    \"stare\": [\"stair\"],\n",
    "    \"stair\": [\"stare\"],\n",
    "    \"stationary\": [\"stationery\"],\n",
    "    \"stationery\": [\"stationary\"],\n",
    "    \"steal\": [\"steel\"],\n",
    "    \"steel\": [\"steal\"],\n",
    "    \"tail\": [\"tale\"],\n",
    "    \"tale\": [\"tail\"],\n",
    "    \"there\": [\"their\", \"they're\"],\n",
    "    \"their\": [\"there\", \"they're\"],\n",
    "    \"they're\": [\"there\", \"their\"],\n",
    "    \"threw\": [\"through\"],\n",
    "    \"through\": [\"threw\"],\n",
    "    \"to\": [\"too\", \"two\"],\n",
    "    \"too\": [\"to\", \"two\"],\n",
    "    \"two\": [\"to\", \"too\"],\n",
    "    \"vain\": [\"vane\", \"vein\"],\n",
    "    \"vane\": [\"vain\", \"vein\"],\n",
    "    \"vein\": [\"vain\", \"vane\"],\n",
    "    \"waste\": [\"waist\"],\n",
    "    \"waist\": [\"waste\"],\n",
    "    \"wait\": [\"weight\"],\n",
    "    \"weight\": [\"wait\"],\n",
    "    \"way\": [\"weigh\", \"whey\"],\n",
    "    \"weigh\": [\"way\", \"whey\"],\n",
    "    \"whey\": [\"way\", \"weigh\"],\n",
    "    \"weak\": [\"week\"],\n",
    "    \"week\": [\"weak\"],\n",
    "    \"wear\": [\"where\"],\n",
    "    \"where\": [\"wear\"],\n",
    "    \"weather\": [\"whether\"],\n",
    "    \"whether\": [\"weather\"],\n",
    "    \"which\": [\"witch\"],\n",
    "    \"witch\": [\"which\"],\n",
    "    \"you're\": [\"your\"],\n",
    "    \"your\": [\"you're\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45d701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional_homophones = {\n",
    "#     \"canvas\": [\"canvass\"],\n",
    "#     \"canvass\": [\"canvas\"],\n",
    "#     \"cereal\": [\"serial\"],\n",
    "#     \"serial\": [\"cereal\"],\n",
    "#     \"chord\": [\"cord\"],\n",
    "#     \"cord\": [\"chord\"],\n",
    "#     \"council\": [\"counsel\"],\n",
    "#     \"counsel\": [\"council\"],\n",
    "#     \"currant\": [\"current\"],\n",
    "#     \"current\": [\"currant\"],\n",
    "#     \"dual\": [\"duel\"],\n",
    "#     \"duel\": [\"dual\"],\n",
    "#     \"gait\": [\"gate\"],\n",
    "#     \"gate\": [\"gait\"],\n",
    "#     \"grate\": [\"great\"],\n",
    "#     \"great\": [\"grate\"],\n",
    "#     \"guessed\": [\"guest\"],\n",
    "#     \"guest\": [\"guessed\"],\n",
    "#     \"holy\": [\"wholly\"],\n",
    "#     \"wholly\": [\"holy\"],\n",
    "#     \"idle\": [\"idol\"],\n",
    "#     \"idol\": [\"idle\"],\n",
    "#     \"leak\": [\"leek\"],\n",
    "#     \"leek\": [\"leak\"],\n",
    "#     \"lessen\": [\"lesson\"],\n",
    "#     \"lesson\": [\"lessen\"],\n",
    "#     \"miner\": [\"minor\"],\n",
    "#     \"minor\": [\"miner\"],\n",
    "#     \"naval\": [\"navel\"],\n",
    "#     \"navel\": [\"naval\"],\n",
    "#     \"pedal\": [\"peddle\"],\n",
    "#     \"peddle\": [\"pedal\"],\n",
    "#     \"peer\": [\"pier\"],\n",
    "#     \"pier\": [\"peer\"],\n",
    "#     \"profit\": [\"prophet\"],\n",
    "#     \"prophet\": [\"profit\"],\n",
    "#     \"root\": [\"route\"],\n",
    "#     \"route\": [\"root\"],\n",
    "#     \"sole\": [\"soul\"],\n",
    "#     \"soul\": [\"sole\"],\n",
    "#     \"stake\": [\"steak\"],\n",
    "#     \"steak\": [\"stake\"],\n",
    "#     \"suite\": [\"sweet\"],\n",
    "#     \"sweet\": [\"suite\"],\n",
    "#     \"vial\": [\"vile\"],\n",
    "#     \"vile\": [\"vial\"],\n",
    "#     \"wail\": [\"whale\"],\n",
    "#     \"whale\": [\"wail\"]\n",
    "# }\n",
    "\n",
    "# # Merge with existing homophones list\n",
    "# homophones.update(additional_homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ebe478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model for POS tagging and dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize BERT fill-mask pipeline\n",
    "fill_mask = pipeline('fill-mask', model='bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb303315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model for POS tagging and dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize BERT fill-mask pipeline\n",
    "fill_mask = pipeline('fill-mask', model='bert-base-cased')\n",
    "\n",
    "# Function to analyze sentence with spaCy (POS and dependency parsing)\n",
    "def analyze_sentence(sentence):\n",
    "    return nlp(sentence)\n",
    "\n",
    "def identify_phrases_for_masking(doc, homophones):\n",
    "    phrases_to_mask = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.text.lower() in homophones:\n",
    "            # Start with the full subtree of the homophone\n",
    "            start = token.left_edge.i\n",
    "            end = token.right_edge.i\n",
    "\n",
    "            # Expand or contract the phrase based on sentence structure\n",
    "            # Handle compound sentences\n",
    "            if token.dep_ in ['conj', 'ccomp']:\n",
    "                start = min(start, token.head.left_edge.i)\n",
    "                end = max(end, token.head.right_edge.i)\n",
    "\n",
    "            # Include modifiers for nouns and verbs\n",
    "            if token.pos_ in ['NOUN', 'PROPN', 'VERB']:\n",
    "                # Include preceding modifiers like adjectives, determiners, auxiliaries, etc.\n",
    "                while start > 0 and doc[start - 1].dep_ in ['amod', 'det', 'nummod', 'aux', 'advmod']:\n",
    "                    start -= 1\n",
    "                # Include objects or complements for verbs\n",
    "                if token.pos_ == 'VERB' and end < len(doc) - 1 and doc[end + 1].dep_ in ['dobj', 'attr', 'prep']:\n",
    "                    end = doc[end + 1].right_edge.i\n",
    "\n",
    "            # Construct the phrase\n",
    "            phrase = doc[start:end + 1].text\n",
    "            phrases_to_mask.append((phrase, token.i))\n",
    "\n",
    "    return phrases_to_mask\n",
    "\n",
    "# Function to create multiple masked versions of the sentence\n",
    "def create_masked_versions(doc, phrases_to_mask):\n",
    "    masked_versions = []\n",
    "\n",
    "    for phrase, phrase_index in phrases_to_mask:\n",
    "        # Decide whether to mask the entire phrase or each word in the phrase\n",
    "        # This decision can be based on the length of the phrase or other criteria\n",
    "        if len(phrase.split()) > 2:  # Example condition: if phrase is longer than 2 words\n",
    "            # Mask the entire phrase with a single [MASK] token\n",
    "            masked_sentence = doc.text.replace(phrase, '[MASK]', 1)\n",
    "            masked_versions.append((masked_sentence, phrase_index))\n",
    "        else:\n",
    "            # Mask each word in the phrase with a [MASK] token\n",
    "            phrase_words = phrase.split()\n",
    "            masked_sentence = [token.text_with_ws for token in doc]\n",
    "            for i in range(len(masked_sentence)):\n",
    "                if phrase_index <= i < phrase_index + len(phrase_words):\n",
    "                    masked_sentence[i] = '[MASK]' + (' ' if masked_sentence[i].endswith(' ') else '')\n",
    "            masked_versions.append((''.join(masked_sentence), phrase_index))\n",
    "\n",
    "    return masked_versions\n",
    "\n",
    "def custom_homophone_replacement(sentence, word, suggestion, index):\n",
    "    # Example: Custom logic for \"four\" and \"for\"\n",
    "    if word == \"four\" and suggestion != \"for\":\n",
    "        # Additional checks can be added here to ensure contextually appropriate replacement\n",
    "        return sentence.replace(word, \"for\", 1)\n",
    "    return sentence.replace(word, suggestion, 1)\n",
    "\n",
    "def sequential_masking_for_homophones(doc, phrases_to_mask):\n",
    "    corrected_sentence = doc.text\n",
    "    for phrase, index in sorted(phrases_to_mask, key=lambda x: x[1]):\n",
    "        # Mask the current phrase\n",
    "        masked_sentence = corrected_sentence.replace(phrase, '[MASK]', 1)\n",
    "        \n",
    "        # Get BERT's prediction for the masked sentence\n",
    "        prediction = fill_mask(masked_sentence)[0]['token_str']\n",
    "        \n",
    "        # Replace [MASK] with the prediction in the corrected sentence\n",
    "        corrected_sentence = corrected_sentence.replace('[MASK]', prediction, 1)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "def analyze_context(sentence, surrounding_text):\n",
    "    # Create a combined document of the surrounding text and the sentence\n",
    "    combined_doc = nlp(surrounding_text + ' ' + sentence)\n",
    "\n",
    "    # NER and Noun Chunks Extraction\n",
    "    entities = [ent.text for ent in combined_doc.ents]\n",
    "    noun_chunks = [chunk.text for chunk in combined_doc.noun_chunks]\n",
    "\n",
    "    # Basic Context Analysis\n",
    "    subjects = [token.text for token in combined_doc if token.dep_ in ['nsubj', 'nsubjpass']]\n",
    "    themes = [chunk.lower() for chunk in noun_chunks if chunk.lower() not in subjects]\n",
    "\n",
    "    # Advanced Topic Modeling (LDA) - # Tokenize the text\n",
    "    tokenized_text = [token.text.lower() for token in combined_doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary([tokenized_text])\n",
    "    \n",
    "    # Convert dictionary to a bag of words corpus\n",
    "    corpus = [dictionary.doc2bow(tokenized_text)]\n",
    "    \n",
    "    # Apply LDA\n",
    "    lda = models.LdaModel(corpus, num_topics=1, id2word=dictionary, passes=10)\n",
    "    lda_topics = lda.print_topics(num_words=3)\n",
    "\n",
    "    # Sentiment Analysis\n",
    "    sentiment = TextBlob(combined_doc.text).sentiment\n",
    "    \n",
    "    # Adding Part-of-Speech (POS) tagging to context analysis\n",
    "    pos_tags = {token.text.lower(): token.pos_ for token in nlp(sentence)}\n",
    "\n",
    "    # Combine All Context Elements\n",
    "    combined_context = {\n",
    "        \"entities\": entities,\n",
    "        \"subjects\": subjects,\n",
    "        \"themes\": themes,\n",
    "        \"lda_topics\": lda_topics,\n",
    "        \"sentiment\": sentiment,\n",
    "        \"pos_tags\": pos_tags\n",
    "    }\n",
    "\n",
    "    return combined_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2417b92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: {'entities': [], 'subjects': ['project', 'team', 'plan'], 'themes': ['the project', 'the team', 'the new plan', 'efficiency'], 'lda_topics': [(0, '0.091*\"team\" + 0.091*\"project\" + 0.091*\"worked\"')], 'sentiment': Sentiment(polarity=0.2111742424242424, subjectivity=0.7490530303030303), 'pos_tags': {'the': 'DET', 'new': 'ADJ', 'plan': 'NOUN', 'was': 'AUX', 'to': 'PART', 'innovate': 'VERB', 'and': 'CCONJ', 'improve': 'VERB', 'efficiency': 'NOUN', '.': 'PUNCT'}}\n"
     ]
    }
   ],
   "source": [
    "surrounding_text = \"The project was challenging but rewarding. The team worked hard.\"\n",
    "sentence = \"The new plan was to innovate and improve efficiency.\"\n",
    "context = analyze_context(sentence, surrounding_text)\n",
    "print(\"Context:\", context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94314814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: The new plan was to perfect the route .\n",
      "Corrections:                      Original Sentence Homophone  Position  \\\n",
      "0  The new plan was to knew the route.       new         1   \n",
      "1  The new plan was to knew the route.        to         4   \n",
      "2  The new plan was to knew the route.      knew         5   \n",
      "\n",
      "                          Corrected Sentence  \n",
      "0  The original plan was to knew the route .  \n",
      "1       The new plan was he knew the route .  \n",
      "2    The new plan was to perfect the route .  \n"
     ]
    }
   ],
   "source": [
    "def correct_homophones_advanced(sentence, surrounding_text=None):\n",
    "    doc = analyze_sentence(sentence)\n",
    "    \n",
    "    # Identify phrases for masking\n",
    "    phrases_to_mask = identify_phrases_for_masking(doc, homophones)\n",
    "    \n",
    "    # Create masked versions of the sentence\n",
    "    masked_versions = create_masked_versions(doc, phrases_to_mask)\n",
    "\n",
    "    table_data = []\n",
    "    \n",
    "    # Sequentially mask and process each homophone\n",
    "    corrected_sentence = sequential_masking_for_homophones(doc, phrases_to_mask)\n",
    "    # corrected_sentence = sentence\n",
    "\n",
    "    for masked_sentence, index in masked_versions:\n",
    "        # Use fill-mask pipeline and decide on replacements\n",
    "        suggestion = fill_mask(masked_sentence)[0]['token_str']\n",
    "\n",
    "        # Replace homophone in the original sentence if suggestion is different\n",
    "        if suggestion.lower() != doc[index].text.lower():\n",
    "            corrected_words = [t.text for t in doc]\n",
    "            corrected_words[index] = suggestion\n",
    "            new_corrected_sentence = ' '.join(corrected_words)\n",
    "            if new_corrected_sentence != corrected_sentence:\n",
    "                corrected_sentence = new_corrected_sentence\n",
    "                table_data.append([sentence, doc[index].text, index, corrected_sentence])\n",
    "\n",
    "    # Context analysis with surrounding text (if provided)\n",
    "    if surrounding_text:\n",
    "        context_elements = analyze_context(sentence, surrounding_text)\n",
    "        # Extract elements from context\n",
    "        entities = context_elements.get(\"entities\", [])\n",
    "        subjects = context_elements.get(\"subjects\", [])\n",
    "        themes = context_elements.get(\"themes\", [])\n",
    "        lda_topics = context_elements.get(\"lda_topics\", [])\n",
    "        sentiment = context_elements.get(\"sentiment\", None)\n",
    "        pos_tags = context_elements.get(\"pos_tags\", {})\n",
    "\n",
    "        # Context-sensitive decision making\n",
    "        for masked_sentence, index in masked_versions:\n",
    "            suggestion = fill_mask(masked_sentence)[0]['token_str']\n",
    "            original_word = doc[index].text.lower()\n",
    "\n",
    "            if suggestion.lower() != original_word:\n",
    "                contextually_relevant = False\n",
    "\n",
    "                # Check if the suggestion matches a subject or theme in the context\n",
    "                if suggestion.lower() in subjects or suggestion.lower() in themes:\n",
    "                    contextually_relevant = True\n",
    "                \n",
    "                # Additional check: Use POS tagging to ensure grammatical consistency\n",
    "                if pos_tags.get(original_word) == pos_tags.get(suggestion.lower()):\n",
    "                    contextually_relevant = True\n",
    "\n",
    "                # Replace homophone only if it's contextually relevant\n",
    "                if contextually_relevant:\n",
    "                    corrected_words = [t.text for t in doc]\n",
    "                    corrected_words[index] = suggestion\n",
    "                    new_corrected_sentence = ' '.join(corrected_words)\n",
    "\n",
    "                    if new_corrected_sentence != corrected_sentence:\n",
    "                        corrected_sentence = new_corrected_sentence\n",
    "                        table_data.append([sentence, original_word, index, corrected_sentence])\n",
    "\n",
    "\n",
    "    # Create a DataFrame for the table\n",
    "    table = pd.DataFrame(table_data, columns=['Original Sentence', 'Homophone', 'Position', 'Corrected Sentence'])\n",
    "\n",
    "    return corrected_sentence, table\n",
    "\n",
    "# Example usage\n",
    "surrounding_text = \"The team was discussing their upcoming project. They were excited about the new opportunities.\"\n",
    "sentence = \"The new plan was to knew the route.\"\n",
    "corrected_sentence, corrections = correct_homophones_advanced(sentence, surrounding_text)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "print(\"Corrections:\", corrections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b8b958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: She wrote the report with great care .\n",
      "Table:\n",
      "                          Original Sentence Homophone  Position  \\\n",
      "0  She wrote there report with great care.     there         2   \n",
      "\n",
      "                       Corrected Sentence  \n",
      "0  She wrote the report with great care .  \n"
     ]
    }
   ],
   "source": [
    "surrounding_text = \"She was always meticulous about her writing. Accuracy mattered to her.\"\n",
    "sentence = \"She wrote there report with great care.\"\n",
    "corrected_sentence, table = correct_homophones_advanced(sentence, surrounding_text)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "print(\"Table:\\n\", table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e94569b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Sentence: Flour power is essential for the baker , but he needs to knead the dough out .\n",
      "Table:\n",
      "                                    Original Sentence Homophone  Position  \\\n",
      "0  Flour power is essential four the baker, but h...     Flour         0   \n",
      "1  Flour power is essential four the baker, but h...      four         4   \n",
      "2  Flour power is essential four the baker, but h...     right        15   \n",
      "\n",
      "                                  Corrected Sentence  \n",
      "0  This power is essential four the baker , but h...  \n",
      "1  Flour power is essential to the baker , but he...  \n",
      "2  Flour power is essential four the baker , but ...  \n"
     ]
    }
   ],
   "source": [
    "surrounding_text = \"The baking competition is next week. Everyone is practicing their recipes.\"\n",
    "sentence = \"Flour power is essential four the baker, but he needs to knead the dough right.\"\n",
    "corrected_sentence, table = correct_homophones_advanced(sentence, surrounding_text)\n",
    "print(\"Corrected Sentence:\", corrected_sentence)\n",
    "print(\"Table:\\n\", table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19da976d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70afd0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to correct homophones and generate output table\n",
    "# def correct_homophones_and_generate_table(sentence, homophones):\n",
    "#     words = sentence.split()\n",
    "#     table_data = []\n",
    "#     corrected_words = words.copy()\n",
    "\n",
    "#     for i, word in enumerate(words):\n",
    "#         lower_word = word.lower()\n",
    "#         if lower_word in homophones:\n",
    "#             print(f\"Detected homophone: {word} at position {i}\")\n",
    "#             masked_sentence = ' '.join(words[:i] + ['[MASK]'] + words[i+1:])\n",
    "#             print(f\"Masked Sentence: {masked_sentence}\")\n",
    "#             suggestion = fill_mask(masked_sentence)[0]['token_str']\n",
    "#             print(f\"BERT's suggestion: {suggestion}\")\n",
    "\n",
    "#             corrected_words[i] = suggestion\n",
    "#             table_data.append([sentence, word, i, ' '.join(corrected_words)])\n",
    "\n",
    "#     table = pd.DataFrame(table_data, columns=['Original Sentence', 'Homophone', 'Position', 'Corrected Sentence'])\n",
    "#     return table, ' '.join(corrected_words)\n",
    "\n",
    "# # Example usage\n",
    "# sentence = \"I new the knew phone was expensive\"\n",
    "# table, corrected_sentence = correct_homophones_and_generate_table(sentence, homophones)\n",
    "\n",
    "# print(\"Corrected Sentence:\", corrected_sentence)\n",
    "# print(\"\\nTable:\")\n",
    "# print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac45fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
