{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Model to Correct Homophone Misue\n",
    "author: Marion Bauman\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our natural language processing model, we will be building a tool that corrects homophone misuse.\n",
    "\n",
    "Our model will compute the likelihood that a given word is correct, and if it is not, it will suggest a replacement word. This will be tested on a corpus of data including some intentional homophone misuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 1.95kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 3.97MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 13.7MB/s]\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 123kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 436M/436M [00:07<00:00, 56.0MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat GPT Prompt: Give me a list of lists in python of 100 sets of homophones\n",
    "homophones_list = [\n",
    "    [\"ate\", \"eight\"],\n",
    "    [\"bare\", \"bear\"],\n",
    "    [\"brake\", \"break\"],\n",
    "    [\"capital\", \"capitol\"],\n",
    "    [\"cell\", \"sell\"],\n",
    "    [\"cite\", \"site\", \"sight\"],\n",
    "    [\"complement\", \"compliment\"],\n",
    "    [\"desert\", \"dessert\"],\n",
    "    [\"die\", \"dye\"],\n",
    "    [\"flour\", \"flower\"],\n",
    "    [\"hear\", \"here\"],\n",
    "    [\"hour\", \"our\"],\n",
    "    [\"knight\", \"night\"],\n",
    "    [\"know\", \"no\"],\n",
    "    [\"mail\", \"male\"],\n",
    "    [\"meat\", \"meet\"],\n",
    "    [\"morning\", \"mourning\"],\n",
    "    [\"one\", \"won\"],\n",
    "    [\"pair\", \"pear\"],\n",
    "    [\"peace\", \"piece\"],\n",
    "    [\"principal\", \"principle\"],\n",
    "    [\"rain\", \"reign\", \"rein\"],\n",
    "    [\"right\", \"write\"],\n",
    "    [\"sea\", \"see\"],\n",
    "    [\"serial\", \"cereal\"],\n",
    "    [\"sole\", \"soul\"],\n",
    "    [\"stationary\", \"stationery\"],\n",
    "    [\"tail\", \"tale\"],\n",
    "    [\"threw\", \"through\"],\n",
    "    [\"to\", \"too\", \"two\"],\n",
    "    [\"weather\", \"whether\"],\n",
    "    [\"week\", \"weak\"],\n",
    "    [\"wear\", \"where\"],\n",
    "    [\"which\", \"witch\"],\n",
    "    [\"your\", \"you're\"],\n",
    "    [\"allowed\", \"aloud\"],\n",
    "    [\"board\", \"bored\"],\n",
    "    [\"brake\", \"break\"],\n",
    "    [\"capital\", \"capitol\"],\n",
    "    [\"compliment\", \"complement\"],\n",
    "    [\"desert\", \"dessert\"],\n",
    "    [\"dual\", \"duel\"],\n",
    "    [\"fair\", \"fare\"],\n",
    "    [\"genre\", \"jinja\"],\n",
    "    [\"hare\", \"hair\"],\n",
    "    [\"here\", \"hear\"],\n",
    "    [\"hoard\", \"horde\"],\n",
    "    [\"loan\", \"lone\"],\n",
    "    [\"pail\", \"pale\"],\n",
    "    [\"peak\", \"peek\", \"pique\"],\n",
    "    [\"profit\", \"prophet\"],\n",
    "    [\"role\", \"roll\"],\n",
    "    [\"root\", \"route\"],\n",
    "    [\"sail\", \"sale\"],\n",
    "    [\"scene\", \"seen\"],\n",
    "    [\"serial\", \"cereal\"],\n",
    "    [\"so\", \"sow\"],\n",
    "    [\"stare\", \"stair\"],\n",
    "    [\"steal\", \"steel\"],\n",
    "    [\"their\", \"there\", \"they're\"],\n",
    "    [\"throne\", \"thrown\"],\n",
    "    [\"vain\", \"vein\", \"vane\"],\n",
    "    [\"weak\", \"week\"],\n",
    "    [\"wood\", \"would\"],\n",
    "    [\"yew\", \"you\"],\n",
    "    [\"bridal\", \"bridle\"],\n",
    "    [\"cereal\", \"serial\"],\n",
    "    [\"chord\", \"cord\"],\n",
    "    [\"compliment\", \"complement\"],\n",
    "    [\"dew\", \"due\"],\n",
    "    [\"foul\", \"fowl\"],\n",
    "    [\"grate\", \"great\"],\n",
    "    [\"groan\", \"grown\"],\n",
    "    [\"heal\", \"heel\"],\n",
    "    [\"him\", \"hymn\"],\n",
    "    [\"lay\", \"lie\"],\n",
    "    [\"main\", \"mane\"],\n",
    "    [\"marry\", \"merry\"],\n",
    "    [\"mite\", \"might\"],\n",
    "    [\"moose\", \"mousse\"],\n",
    "    [\"mourn\", \"morn\"],\n",
    "    [\"peace\", \"piece\"],\n",
    "    [\"plum\", \"plumb\"],\n",
    "    [\"pour\", \"pore\"],\n",
    "    [\"rap\", \"wrap\"],\n",
    "    [\"scene\", \"seen\"],\n",
    "    [\"scent\", \"cent\", \"sent\"],\n",
    "    [\"serial\", \"cereal\"],\n",
    "    [\"shear\", \"sheer\"],\n",
    "    [\"soar\", \"sore\"],\n",
    "    [\"sow\", \"sew\"],\n",
    "    [\"stake\", \"steak\"],\n",
    "    [\"tide\", \"tied\"],\n",
    "    [\"toe\", \"tow\"],\n",
    "    [\"there\", \"their\", \"they're\"],\n",
    "    [\"waist\", \"waste\"],\n",
    "    [\"week\", \"weak\"],\n",
    "    [\"write\", \"right\", \"rite\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gutenberg_homophone_data = pd.read_csv('../data/gutenberg-homophone-errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = gutenberg_homophone_data['sentences'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gut', '##enberg', 'e', '##book', 'of', 'f', '##rank', '##enstein', ';', 'or', ',', 'the', 'modern', 'pro', '##met', '##heus', 'this', 'e', '##book', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', '##most', 'other', 'parts', 'of', 'the', 'world', 'at', 'know', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', '##w', '##hat', '##so', '##ever', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.tokenize(test_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of lists\n",
    "homophones_list_flat = [item for sublist in homophones_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "know\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no\n"
     ]
    }
   ],
   "source": [
    "outs = []\n",
    "for ts in range(1, len(tokenized_sentence)):\n",
    "    ts_full = tokenized_sentence[0:ts+1]\n",
    "    if ts_full[ts] in homophones_list_flat:\n",
    "        print(ts_full[ts])\n",
    "        ts_full[ts] = '[MASK]'\n",
    "        tokenize_ts_full = tokenizer(ts_full, return_tensors='pt', padding=True, truncation=True)\n",
    "        model_output = model(**tokenize_ts_full)\n",
    "        outs.append(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MaskedLMOutput(loss=None, logits=tensor([[[ -7.2337,  -7.1655,  -7.2393,  ...,  -5.9906,  -5.7929,  -6.1766],\n",
       "          [ -7.8210,  -7.9910,  -7.8512,  ...,  -6.4410,  -6.3874,  -6.7109],\n",
       "          [-10.6334, -10.4260, -10.4234,  ...,  -8.4052,  -9.2172,  -9.4748],\n",
       "          [ -5.6448,  -5.4847,  -5.7676,  ...,  -3.5781,  -5.0492,  -4.3255],\n",
       "          [ -5.3552,  -5.2033,  -5.4628,  ...,  -3.5359,  -4.8507,  -4.1003],\n",
       "          [ -5.4304,  -5.3131,  -5.5703,  ...,  -3.4580,  -5.1538,  -4.4302]],\n",
       " \n",
       "         [[ -7.2990,  -7.1915,  -7.2768,  ...,  -6.1238,  -5.7690,  -6.2951],\n",
       "          [ -7.9527,  -8.1324,  -7.9212,  ...,  -6.6422,  -6.5342,  -7.0476],\n",
       "          [-11.1965, -11.2232, -11.4031,  ...,  -8.1919, -10.1863, -10.9385],\n",
       "          [ -4.9157,  -4.6635,  -4.8502,  ...,  -3.1893,  -4.0294,  -5.2144],\n",
       "          [ -4.6782,  -4.4324,  -4.6052,  ...,  -3.2563,  -3.8875,  -5.3541],\n",
       "          [ -5.0233,  -4.7579,  -5.0839,  ...,  -3.6479,  -4.0018,  -5.2299]],\n",
       " \n",
       "         [[ -7.2721,  -7.2275,  -7.3099,  ...,  -6.0273,  -5.8526,  -6.2322],\n",
       "          [ -6.5307,  -6.5940,  -6.6484,  ...,  -4.7478,  -5.8477,  -5.6384],\n",
       "          [ -9.1830,  -9.5739,  -9.2836,  ...,  -7.3609,  -9.1539,  -8.4830],\n",
       "          [ -5.3244,  -5.4817,  -5.4833,  ...,  -4.1322,  -4.7098,  -4.3288],\n",
       "          [ -5.2702,  -5.4322,  -5.3976,  ...,  -4.2681,  -4.6404,  -4.4059],\n",
       "          [ -5.1758,  -5.1608,  -5.3059,  ...,  -4.0288,  -4.8105,  -3.9489]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -7.4906,  -7.4250,  -7.5424,  ...,  -6.2707,  -6.0404,  -6.5538],\n",
       "          [ -7.8525,  -8.0023,  -7.8513,  ...,  -6.6113,  -6.3759,  -7.0548],\n",
       "          [-12.0599, -12.2888, -11.5900,  ...,  -9.6636,  -9.9089, -11.2888],\n",
       "          [ -6.2067,  -6.2744,  -6.3020,  ...,  -4.3707,  -5.5911,  -6.0149],\n",
       "          [ -6.1296,  -6.2289,  -6.2586,  ...,  -4.4635,  -5.6026,  -6.1440],\n",
       "          [ -6.1638,  -6.1543,  -6.4056,  ...,  -4.2697,  -5.9795,  -6.0224]],\n",
       " \n",
       "         [[ -7.2715,  -7.2372,  -7.3289,  ...,  -6.0646,  -5.7798,  -6.3068],\n",
       "          [ -7.7204,  -7.8747,  -7.6879,  ...,  -6.3602,  -6.2984,  -6.8339],\n",
       "          [ -8.6621,  -8.7618,  -8.7561,  ...,  -7.3121,  -7.7348,  -8.4073],\n",
       "          [ -5.3551,  -5.2414,  -5.4101,  ...,  -3.9478,  -4.7992,  -5.2064],\n",
       "          [ -5.0593,  -4.9274,  -5.1011,  ...,  -3.7932,  -4.6176,  -4.9979],\n",
       "          [ -5.3525,  -5.1998,  -5.3976,  ...,  -3.8720,  -5.0594,  -5.1946]],\n",
       " \n",
       "         [[ -6.2618,  -6.4497,  -6.4353,  ...,  -5.2509,  -5.2181,  -6.0629],\n",
       "          [ -7.0092,  -7.1000,  -7.0920,  ...,  -5.4472,  -5.8453,  -5.7803],\n",
       "          [ -9.1096,  -9.6365,  -9.2802,  ...,  -7.5104,  -7.5684,  -7.9724],\n",
       "          [ -6.5315,  -6.4890,  -6.5619,  ...,  -4.6220,  -5.2595,  -4.9646],\n",
       "          [ -6.5424,  -6.5553,  -6.6277,  ...,  -4.7281,  -5.2470,  -5.1855],\n",
       "          [ -6.0643,  -5.8389,  -6.0570,  ...,  -4.1305,  -5.1404,  -4.4053]]],\n",
       "        grad_fn=<ViewBackward0>), hidden_states=None, attentions=None),\n",
       " MaskedLMOutput(loss=None, logits=tensor([[[ -7.2337,  -7.1655,  -7.2393,  ...,  -5.9906,  -5.7929,  -6.1766],\n",
       "          [ -7.8210,  -7.9910,  -7.8512,  ...,  -6.4410,  -6.3874,  -6.7109],\n",
       "          [-10.6334, -10.4260, -10.4234,  ...,  -8.4052,  -9.2172,  -9.4748],\n",
       "          [ -5.6448,  -5.4847,  -5.7676,  ...,  -3.5781,  -5.0492,  -4.3255],\n",
       "          [ -5.3552,  -5.2033,  -5.4628,  ...,  -3.5359,  -4.8507,  -4.1003],\n",
       "          [ -5.4304,  -5.3131,  -5.5703,  ...,  -3.4580,  -5.1538,  -4.4302]],\n",
       " \n",
       "         [[ -7.2990,  -7.1915,  -7.2768,  ...,  -6.1238,  -5.7690,  -6.2951],\n",
       "          [ -7.9527,  -8.1324,  -7.9212,  ...,  -6.6422,  -6.5342,  -7.0476],\n",
       "          [-11.1965, -11.2232, -11.4031,  ...,  -8.1919, -10.1863, -10.9385],\n",
       "          [ -4.9157,  -4.6635,  -4.8502,  ...,  -3.1893,  -4.0294,  -5.2144],\n",
       "          [ -4.6782,  -4.4324,  -4.6052,  ...,  -3.2563,  -3.8875,  -5.3541],\n",
       "          [ -5.0233,  -4.7579,  -5.0839,  ...,  -3.6479,  -4.0018,  -5.2299]],\n",
       " \n",
       "         [[ -7.2721,  -7.2275,  -7.3099,  ...,  -6.0273,  -5.8526,  -6.2322],\n",
       "          [ -6.5307,  -6.5940,  -6.6484,  ...,  -4.7478,  -5.8477,  -5.6384],\n",
       "          [ -9.1830,  -9.5739,  -9.2836,  ...,  -7.3609,  -9.1539,  -8.4830],\n",
       "          [ -5.3244,  -5.4817,  -5.4833,  ...,  -4.1322,  -4.7098,  -4.3288],\n",
       "          [ -5.2702,  -5.4322,  -5.3976,  ...,  -4.2681,  -4.6404,  -4.4059],\n",
       "          [ -5.1758,  -5.1608,  -5.3059,  ...,  -4.0288,  -4.8105,  -3.9489]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ -7.1015,  -6.9927,  -7.1148,  ...,  -5.9280,  -5.6708,  -6.0260],\n",
       "          [ -7.8212,  -7.9892,  -7.8345,  ...,  -6.4912,  -6.3348,  -6.7712],\n",
       "          [ -9.0157,  -9.3428,  -9.1948,  ...,  -8.4446,  -8.9790,  -8.3305],\n",
       "          [ -5.2615,  -5.2269,  -5.5422,  ...,  -3.9264,  -5.5264,  -4.9635],\n",
       "          [ -5.1160,  -5.0644,  -5.3800,  ...,  -3.7853,  -5.3745,  -4.7270],\n",
       "          [ -5.4267,  -5.2477,  -5.7358,  ...,  -3.8381,  -5.7594,  -5.2619]],\n",
       " \n",
       "         [[ -7.3049,  -7.1880,  -7.3584,  ...,  -6.1183,  -5.8794,  -6.1432],\n",
       "          [ -7.0977,  -7.2434,  -7.0520,  ...,  -5.7350,  -6.0008,  -6.0596],\n",
       "          [ -9.8332, -10.2737,  -9.9139,  ...,  -8.4977,  -8.5370,  -8.5347],\n",
       "          [ -4.8535,  -4.6718,  -4.7954,  ...,  -3.8803,  -3.9721,  -3.2589],\n",
       "          [ -4.6592,  -4.4614,  -4.5911,  ...,  -3.8103,  -3.8110,  -3.1702],\n",
       "          [ -4.6921,  -4.2490,  -4.6176,  ...,  -3.8030,  -4.0914,  -3.4380]],\n",
       " \n",
       "         [[ -6.2618,  -6.4497,  -6.4353,  ...,  -5.2509,  -5.2181,  -6.0629],\n",
       "          [ -7.0092,  -7.1000,  -7.0920,  ...,  -5.4472,  -5.8453,  -5.7803],\n",
       "          [ -9.1096,  -9.6365,  -9.2802,  ...,  -7.5104,  -7.5684,  -7.9724],\n",
       "          [ -6.5315,  -6.4890,  -6.5619,  ...,  -4.6220,  -5.2595,  -4.9646],\n",
       "          [ -6.5424,  -6.5553,  -6.6277,  ...,  -4.7281,  -5.2470,  -5.1855],\n",
       "          [ -6.0643,  -5.8389,  -6.0570,  ...,  -4.1305,  -5.1404,  -4.4053]]],\n",
       "        grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
