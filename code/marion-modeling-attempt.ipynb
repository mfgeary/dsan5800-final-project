{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Model to Correct Homophone Misue\n",
    "author: Marion Bauman\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our natural language processing model, we will be building a tool that corrects homophone misuse.\n",
    "\n",
    "Our model will compute the likelihood that a given word is correct, and if it is not, it will suggest a replacement word. This will be tested on a corpus of data including some intentional homophone misuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tokenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<00:00, 1.95kB/s]\n",
      "Downloading vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 3.97MB/s]\n",
      "Downloading tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 13.7MB/s]\n",
      "Downloading config.json: 100%|██████████| 570/570 [00:00<00:00, 123kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 436M/436M [00:07<00:00, 56.0MB/s] \n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat GPT Prompt: Give me a list of lists in python of 100 sets of homophones\n",
    "homophones_list = [\n",
    "    [\"ate\", \"eight\"],\n",
    "    [\"bare\", \"bear\"],\n",
    "    [\"brake\", \"break\"],\n",
    "    [\"capital\", \"capitol\"],\n",
    "    [\"cell\", \"sell\"],\n",
    "    [\"cite\", \"site\", \"sight\"],\n",
    "    [\"complement\", \"compliment\"],\n",
    "    [\"desert\", \"dessert\"],\n",
    "    [\"die\", \"dye\"],\n",
    "    [\"flour\", \"flower\"],\n",
    "    [\"hear\", \"here\"],\n",
    "    [\"hour\", \"our\"],\n",
    "    [\"knight\", \"night\"],\n",
    "    [\"know\", \"no\"],\n",
    "    [\"mail\", \"male\"],\n",
    "    [\"meat\", \"meet\"],\n",
    "    [\"morning\", \"mourning\"],\n",
    "    [\"one\", \"won\"],\n",
    "    [\"pair\", \"pear\"],\n",
    "    [\"peace\", \"piece\"],\n",
    "    [\"principal\", \"principle\"],\n",
    "    [\"rain\", \"reign\", \"rein\"],\n",
    "    [\"right\", \"write\"],\n",
    "    [\"sea\", \"see\"],\n",
    "    [\"serial\", \"cereal\"],\n",
    "    [\"sole\", \"soul\"],\n",
    "    [\"stationary\", \"stationery\"],\n",
    "    [\"tail\", \"tale\"],\n",
    "    [\"threw\", \"through\"],\n",
    "    [\"to\", \"too\", \"two\"],\n",
    "    [\"weather\", \"whether\"],\n",
    "    [\"week\", \"weak\"],\n",
    "    [\"wear\", \"where\"],\n",
    "    [\"which\", \"witch\"],\n",
    "    [\"your\", \"you're\"],\n",
    "    [\"allowed\", \"aloud\"],\n",
    "    [\"board\", \"bored\"],\n",
    "    [\"brake\", \"break\"],\n",
    "    [\"capital\", \"capitol\"],\n",
    "    [\"compliment\", \"complement\"],\n",
    "    [\"desert\", \"dessert\"],\n",
    "    [\"dual\", \"duel\"],\n",
    "    [\"fair\", \"fare\"],\n",
    "    [\"genre\", \"jinja\"],\n",
    "    [\"hare\", \"hair\"],\n",
    "    [\"here\", \"hear\"],\n",
    "    [\"hoard\", \"horde\"],\n",
    "    [\"loan\", \"lone\"],\n",
    "    [\"pail\", \"pale\"],\n",
    "    [\"peak\", \"peek\", \"pique\"],\n",
    "    [\"profit\", \"prophet\"],\n",
    "    [\"role\", \"roll\"],\n",
    "    [\"root\", \"route\"],\n",
    "    [\"sail\", \"sale\"],\n",
    "    [\"scene\", \"seen\"],\n",
    "    [\"serial\", \"cereal\"],\n",
    "    [\"so\", \"sow\"],\n",
    "    [\"stare\", \"stair\"],\n",
    "    [\"steal\", \"steel\"],\n",
    "    [\"their\", \"there\", \"they're\"],\n",
    "    [\"throne\", \"thrown\"],\n",
    "    [\"vain\", \"vein\", \"vane\"],\n",
    "    [\"weak\", \"week\"],\n",
    "    [\"wood\", \"would\"],\n",
    "    [\"yew\", \"you\"],\n",
    "    [\"bridal\", \"bridle\"],\n",
    "    [\"cereal\", \"serial\"],\n",
    "    [\"chord\", \"cord\"],\n",
    "    [\"compliment\", \"complement\"],\n",
    "    [\"dew\", \"due\"],\n",
    "    [\"foul\", \"fowl\"],\n",
    "    [\"grate\", \"great\"],\n",
    "    [\"groan\", \"grown\"],\n",
    "    [\"heal\", \"heel\"],\n",
    "    [\"him\", \"hymn\"],\n",
    "    [\"lay\", \"lie\"],\n",
    "    [\"main\", \"mane\"],\n",
    "    [\"marry\", \"merry\"],\n",
    "    [\"mite\", \"might\"],\n",
    "    [\"moose\", \"mousse\"],\n",
    "    [\"mourn\", \"morn\"],\n",
    "    [\"peace\", \"piece\"],\n",
    "    [\"plum\", \"plumb\"],\n",
    "    [\"pour\", \"pore\"],\n",
    "    [\"rap\", \"wrap\"],\n",
    "    [\"scene\", \"seen\"],\n",
    "    [\"scent\", \"cent\", \"sent\"],\n",
    "    [\"serial\", \"cereal\"],\n",
    "    [\"shear\", \"sheer\"],\n",
    "    [\"soar\", \"sore\"],\n",
    "    [\"sow\", \"sew\"],\n",
    "    [\"stake\", \"steak\"],\n",
    "    [\"tide\", \"tied\"],\n",
    "    [\"toe\", \"tow\"],\n",
    "    [\"there\", \"their\", \"they're\"],\n",
    "    [\"waist\", \"waste\"],\n",
    "    [\"week\", \"weak\"],\n",
    "    [\"write\", \"right\", \"rite\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gutenberg_homophone_data = pd.read_csv('../data/gutenberg-homophone-errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"He doesn't no how to code.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'doesn', \"'\", 't', 'no', 'how', 'to', 'code', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = tokenizer.tokenize(test_sentence)\n",
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of lists\n",
    "homophones_list_flat = [item for sublist in homophones_list for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "PipelineException",
     "evalue": "No mask_token ([MASK]) found on the input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mariongeary/Library/Mobile Documents/com~apple~CloudDocs/Documents/grad_school/s1/dsan5800/final-project/dsan5800-final-project/code/marion-modeling-attempt.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariongeary/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/grad_school/s1/dsan5800/final-project/dsan5800-final-project/code/marion-modeling-attempt.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mariongeary/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/grad_school/s1/dsan5800/final-project/dsan5800-final-project/code/marion-modeling-attempt.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mask_filler \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39m\u001b[39mfill-mask\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbert-base-cased\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mariongeary/Library/Mobile%20Documents/com~apple~CloudDocs/Documents/grad_school/s1/dsan5800/final-project/dsan5800-final-project/code/marion-modeling-attempt.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m mask_filler(tokenized_sentence, top_k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py:270\u001b[0m, in \u001b[0;36mFillMaskPipeline.__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    249\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m    Fill the masked token in the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m        - **token_str** (`str`) -- The predicted token (to replace the masked one).\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(inputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(final_iterator)\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/pt_utils.py:19\u001b[0m, in \u001b[0;36mPipelineDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m     18\u001b[0m     item \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[i]\n\u001b[0;32m---> 19\u001b[0m     processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py:123\u001b[0m, in \u001b[0;36mFillMaskPipeline.preprocess\u001b[0;34m(self, inputs, return_tensors, tokenizer_kwargs, **preprocess_parameters)\u001b[0m\n\u001b[1;32m    120\u001b[0m     tokenizer_kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m    122\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer(inputs, return_tensors\u001b[39m=\u001b[39mreturn_tensors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenizer_kwargs)\n\u001b[0;32m--> 123\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mensure_exactly_one_mask_token(model_inputs)\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m model_inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py:112\u001b[0m, in \u001b[0;36mFillMaskPipeline.ensure_exactly_one_mask_token\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[39mfor\u001b[39;00m input_ids \u001b[39min\u001b[39;00m model_inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m--> 112\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_exactly_one_mask_token(input_ids)\n",
      "File \u001b[0;32m~/anaconda3/envs/dsan5800/lib/python3.9/site-packages/transformers/pipelines/fill_mask.py:100\u001b[0m, in \u001b[0;36mFillMaskPipeline._ensure_exactly_one_mask_token\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     98\u001b[0m numel \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mprod(masked_index\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m numel \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mraise\u001b[39;00m PipelineException(\n\u001b[1;32m    101\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfill-mask\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    102\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mbase_model_prefix,\n\u001b[1;32m    103\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo mask_token (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mmask_token\u001b[39m}\u001b[39;00m\u001b[39m) found on the input\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m     )\n",
      "\u001b[0;31mPipelineException\u001b[0m: No mask_token ([MASK]) found on the input"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask_filler = pipeline(\"fill-mask\", \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He']\n",
      "['He', \"doesn't\"]\n",
      "['He', \"doesn't\", 'no']\n",
      "no\n",
      "He doesn't [MASK]\n",
      "[{'score': 0.9814466238021851, 'token': 119, 'token_str': '.', 'sequence': \"He doesn't.\"}, {'score': 0.009747138246893883, 'token': 132, 'token_str': ';', 'sequence': \"He doesn't ;\"}, {'score': 0.00484459986910224, 'token': 106, 'token_str': '!', 'sequence': \"He doesn't!\"}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', \"doesn't\", 'no', 'how']\n",
      "['He', \"doesn't\", 'no', 'how', 'to']\n",
      "to\n",
      "He doesn't no how [MASK]\n",
      "[{'score': 0.7354617714881897, 'token': 119, 'token_str': '.', 'sequence': \"He doesn't no how.\"}, {'score': 0.1854487657546997, 'token': 136, 'token_str': '?', 'sequence': \"He doesn't no how?\"}, {'score': 0.055269792675971985, 'token': 106, 'token_str': '!', 'sequence': \"He doesn't no how!\"}]\n",
      "['He', \"doesn't\", 'no', 'how', 'to', 'code.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "testy = test_sentence.split()\n",
    "outs = []\n",
    "for ts in range(0, len(testy)):\n",
    "    ts_full = testy.copy()[:ts+1]\n",
    "    print(ts_full)\n",
    "    if ts_full[ts] in homophones_list_flat:\n",
    "        print(ts_full[ts])\n",
    "        ts_full[ts] = '[MASK]'\n",
    "        ts_full = ' '.join(ts_full)\n",
    "        print(ts_full)\n",
    "        print(mask_filler(ts_full, top_k=3))\n",
    "        outs.append(mask_filler(ts_full, top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.9814466238021851,\n",
       "   'token': 119,\n",
       "   'token_str': '.',\n",
       "   'sequence': \"He doesn't.\"},\n",
       "  {'score': 0.009747138246893883,\n",
       "   'token': 132,\n",
       "   'token_str': ';',\n",
       "   'sequence': \"He doesn't ;\"},\n",
       "  {'score': 0.00484459986910224,\n",
       "   'token': 106,\n",
       "   'token_str': '!',\n",
       "   'sequence': \"He doesn't!\"},\n",
       "  {'score': 0.003609249135479331,\n",
       "   'token': 136,\n",
       "   'token_str': '?',\n",
       "   'sequence': \"He doesn't?\"},\n",
       "  {'score': 0.0001153292105300352,\n",
       "   'token': 1232,\n",
       "   'token_str': '...',\n",
       "   'sequence': \"He doesn't...\"}],\n",
       " [{'score': 0.7354617714881897,\n",
       "   'token': 119,\n",
       "   'token_str': '.',\n",
       "   'sequence': \"He doesn't no how.\"},\n",
       "  {'score': 0.1854487657546997,\n",
       "   'token': 136,\n",
       "   'token_str': '?',\n",
       "   'sequence': \"He doesn't no how?\"},\n",
       "  {'score': 0.055269792675971985,\n",
       "   'token': 106,\n",
       "   'token_str': '!',\n",
       "   'sequence': \"He doesn't no how!\"},\n",
       "  {'score': 0.01971513405442238,\n",
       "   'token': 132,\n",
       "   'token_str': ';',\n",
       "   'sequence': \"He doesn't no how ;\"},\n",
       "  {'score': 0.0010626193834468722,\n",
       "   'token': 1232,\n",
       "   'token_str': '...',\n",
       "   'sequence': \"He doesn't no how...\"}]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_homophones = [\n",
    "    ['accessary', 'accessory'],\n",
    "    ['ad', 'add'],\n",
    "    ['ail', 'ale'],\n",
    "    ['air', 'heir'],\n",
    "    ['aisle', \"I'll\", 'isle'],\n",
    "    ['all', 'awl'],\n",
    "    ['allowed', 'aloud'],\n",
    "    ['altar', 'alter'],\n",
    "    ['arc', 'ark'],\n",
    "    ['ant', 'aunt'],\n",
    "    ['ate', 'eight'],\n",
    "    ['auger', 'augur'],\n",
    "    ['auk', 'orc'],\n",
    "    ['aural', 'oral'],\n",
    "    ['away', 'aweigh'],\n",
    "    ['aw', 'awe'],\n",
    "    ['ore', 'oar', 'or'],\n",
    "    ['axel', 'axle'],\n",
    "    ['aye', 'eye', 'I'],\n",
    "    ['bail', 'bale'],\n",
    "    ['bait', 'bate'],\n",
    "    ['baize', 'bays'],\n",
    "    ['bald', 'bawled'],\n",
    "    ['ball', 'bawl'],\n",
    "    ['band', 'banned'],\n",
    "    ['bard', 'barred'],\n",
    "    ['bare', 'bear'],\n",
    "    ['bark', 'barque'],\n",
    "    ['baron', 'barren'],\n",
    "    ['base', 'bass'],\n",
    "    ['based', 'baste'],\n",
    "    ['bazaar', 'bizarre'],\n",
    "    ['be', 'bee'],\n",
    "    ['bay', 'bey'],\n",
    "    ['beach', 'beech'],\n",
    "    ['bean', 'been'],\n",
    "    ['beat', 'beet'],\n",
    "    ['beau', 'bow'],\n",
    "    ['beer', 'bier'],\n",
    "    ['bel', 'bell', 'belle'],\n",
    "    ['berry', 'bury'],\n",
    "    ['berth', 'birth'],\n",
    "    ['bight', 'bite', 'byte'],\n",
    "    ['billed', 'build'],\n",
    "    ['bitten', 'bittern'],\n",
    "    ['blew', 'blue'],\n",
    "    ['bloc', 'block', 'bloque'],\n",
    "    ['boar', 'bore'],\n",
    "    ['board', 'bored'],\n",
    "    ['boarder', 'border'],\n",
    "    ['bold', 'bowled'],\n",
    "    ['boos', 'booze'],\n",
    "    ['born', 'borne'],\n",
    "    ['bough', 'bow'],\n",
    "    ['boy', 'buoy'],\n",
    "    ['brae', 'bray'],\n",
    "    ['braid', 'brayed'],\n",
    "    ['braise', 'brays', 'braze'],\n",
    "    ['brake', 'break'],\n",
    "    ['bread', 'bred'],\n",
    "    ['brews', 'bruise'],\n",
    "    ['bridal', 'bridle'],\n",
    "    ['broach', 'brooch'],\n",
    "    ['bur', 'burr', 'brr'],\n",
    "    ['but', 'butt'],\n",
    "    ['buy', 'by', 'bye'],\n",
    "    ['buyer', 'byre'],\n",
    "    ['calendar', 'calender'],\n",
    "    ['call', 'caul'],\n",
    "    ['canvas', 'canvass'],\n",
    "    ['cast', 'caste'],\n",
    "    ['caster', 'castor'],\n",
    "    ['caught', 'court'],\n",
    "    ['caw', 'core', 'corps'],\n",
    "    ['cede', 'seed'],\n",
    "    ['ceiling', 'sealing'],\n",
    "    ['cell', 'sell'],\n",
    "    ['censer', 'censor', 'sensor'],\n",
    "    ['cent', 'scent', 'sent'],\n",
    "    ['cereal', 'serial'],\n",
    "    ['cheap', 'cheep'],\n",
    "    ['check', 'cheque'],\n",
    "    ['choir', 'quire'],\n",
    "    ['chord', 'cord'],\n",
    "    ['cite', 'sight', 'site'],\n",
    "    ['clack', 'claque'],\n",
    "    ['clew', 'clue'],\n",
    "    ['climb', 'clime'],\n",
    "    ['close', 'cloze'],\n",
    "    ['coal', 'kohl'],\n",
    "    ['coarse', 'course'],\n",
    "    ['coign', 'coin'],\n",
    "    ['colonel', 'kernel'],\n",
    "    ['complacent', 'complaisant'],\n",
    "    ['complement', 'compliment'],\n",
    "    ['coo', 'coup'],\n",
    "    ['cops', 'copse'],\n",
    "    ['council', 'counsel'],\n",
    "    ['cousin', 'cozen'],\n",
    "    ['creak', 'creek'],\n",
    "    ['crews', 'cruise'],\n",
    "    ['cue', 'kyu', 'queue'],\n",
    "    ['curb', 'kerb'],\n",
    "    ['currant', 'current'],\n",
    "    ['cymbol', 'symbol'],\n",
    "    ['dam', 'damn'],\n",
    "    ['days', 'daze'],\n",
    "    ['dear', 'deer'],\n",
    "    ['descent', 'dissent'],\n",
    "    ['desert', 'dessert'],\n",
    "    ['deviser', 'divisor'],\n",
    "    ['dew', 'due'],\n",
    "    ['die', 'dye'],\n",
    "    ['discreet', 'discrete'],\n",
    "    ['doe', 'doh', 'dough'],\n",
    "    ['done', 'dun'],\n",
    "    ['douse', 'dowse'],\n",
    "    ['draft', 'draught'],\n",
    "    ['dual', 'duel'],\n",
    "    ['earn', 'urn'],\n",
    "    ['eery', 'eyrie'],\n",
    "    ['ewe', 'yew', 'you'],\n",
    "    ['faint', 'feint'],\n",
    "    ['fah', 'far'],\n",
    "    ['fair', 'fare'],\n",
    "    ['fairy', 'ferry'],\n",
    "    ['fate', 'fete'],\n",
    "    ['farther', 'father'],\n",
    "    ['faun', 'fawn'],\n",
    "    ['faze', 'phase'],\n",
    "    ['fay', 'fey'],\n",
    "    ['feat', 'feet'],\n",
    "    ['ferrule', 'ferule'],\n",
    "    ['few', 'phew'],\n",
    "    ['fie', 'phi'],\n",
    "    ['file', 'phial'],\n",
    "    ['find', 'fined'],\n",
    "    ['fir', 'fur'],\n",
    "    ['fizz', 'phiz'],\n",
    "    ['flair', 'flare'],\n",
    "    ['flaw', 'floor'],\n",
    "    ['flea', 'flee'],\n",
    "    ['flex', 'flecks'],\n",
    "    ['flew', 'flu', 'flue'],\n",
    "    ['floe', 'flow'],\n",
    "    ['flour', 'flower'],\n",
    "    ['for', 'fore', 'four'],\n",
    "    ['foreword', 'forward'],\n",
    "    ['fort', 'fought'],\n",
    "    ['forth', 'fourth'],\n",
    "    ['foul', 'fowl'],\n",
    "    ['franc', 'frank'],\n",
    "    ['freeze', 'frieze'],\n",
    "    ['friar', 'fryer'],\n",
    "    ['furs', 'furze'],\n",
    "    ['gait', 'gate'],\n",
    "    ['galipot', 'gallipot'],\n",
    "    ['gamble', 'gambol'],\n",
    "    ['gallop', 'galop'],\n",
    "    ['gays', 'gaze'],\n",
    "    ['genes', 'jeans'],\n",
    "    ['gild', 'guild'],\n",
    "    ['gilt', 'guilt'],\n",
    "    ['giro', 'gyro'],\n",
    "    ['gnaw', 'nor'],\n",
    "    ['gneiss', 'nice'],\n",
    "    ['gorilla', 'guerilla'],\n",
    "    ['grate', 'great'],\n",
    "    ['greave', 'grieve'],\n",
    "    ['greys', 'graze'],\n",
    "    ['grisly', 'grizzly'],\n",
    "    ['groan', 'grown'],\n",
    "    ['guessed', 'guest'],\n",
    "    ['hail', 'hale'],\n",
    "    ['hair', 'hare'],\n",
    "    ['hall', 'haul'],\n",
    "    ['hangar', 'hanger'],\n",
    "    ['hart', 'heart'],\n",
    "    ['haw', 'hoar', 'whore'],\n",
    "    ['hay', 'hey'],\n",
    "    ['heal', 'heel', \"he'll\"],\n",
    "    ['here', 'hear'],\n",
    "    ['heard', 'herd'],\n",
    "    [\"he'd\", 'heed'],\n",
    "    ['heroin', 'heroine'],\n",
    "    ['hew', 'hue'],\n",
    "    ['hi', 'high'],\n",
    "    ['higher', 'hire'],\n",
    "    ['him', 'hymn'],\n",
    "    ['ho', 'hoe'],\n",
    "    ['hoard', 'horde'],\n",
    "    ['hoarse', 'horse'],\n",
    "    ['holey', 'holy', 'wholly'],\n",
    "    ['hour', 'our'],\n",
    "    ['idle', 'idol'],\n",
    "    ['in', 'inn'],\n",
    "    ['indict', 'indite'],\n",
    "    [\"it's\", 'its'],\n",
    "    ['jewel', 'joule', 'juul'],\n",
    "    ['key', 'quay'],\n",
    "    ['knave', 'nave'],\n",
    "    ['knead', 'need'],\n",
    "    ['knew', 'new'],\n",
    "    ['knight', 'night'],\n",
    "    ['knit', 'nit'],\n",
    "    ['knob', 'nob'],\n",
    "    ['knock', 'nock'],\n",
    "    ['knot', 'not'],\n",
    "    ['know', 'no'],\n",
    "    ['knows', 'nose'],\n",
    "    ['laager', 'lager'],\n",
    "    ['lac', 'lack'],\n",
    "    ['lade', 'laid'],\n",
    "    ['lain', 'lane'],\n",
    "    ['lam', 'lamb'],\n",
    "    ['laps', 'lapse'],\n",
    "    ['larva', 'lava'],\n",
    "    ['lase', 'laze'],\n",
    "    ['law', 'lore'],\n",
    "    ['lay', 'ley'],\n",
    "    ['lea', 'lee'],\n",
    "    ['leach', 'leech'],\n",
    "    ['lead', 'led'],\n",
    "    ['leak', 'leek'],\n",
    "    ['lean', 'lien'],\n",
    "    ['lessen', 'lesson'],\n",
    "    ['levee', 'levy'],\n",
    "    ['liar', 'lyre'],\n",
    "    ['licence', 'license'],\n",
    "    ['licker', 'liquor'],\n",
    "    ['lie', 'lye'],\n",
    "    ['lieu', 'loo'],\n",
    "    ['links', 'lynx'],\n",
    "    ['lo', 'low'],\n",
    "    ['load', 'lode'],\n",
    "    ['loan', 'lone'],\n",
    "    ['locks', 'lox'],\n",
    "    ['loop', 'loupe'],\n",
    "    ['loot', 'lute'],\n",
    "    ['made', 'maid'],\n",
    "    ['mail', 'male'],\n",
    "    ['main', 'mane'],\n",
    "    ['maize', 'maze'],\n",
    "    ['mall', 'maul'],\n",
    "    ['manna', 'manner'],\n",
    "    ['mantel', 'mantle'],\n",
    "    ['mare', 'mayor'],\n",
    "    ['mark', 'marque'],\n",
    "    ['marshal', 'martial'],\n",
    "    ['marten', 'martin'],\n",
    "    ['mask', 'masque'],\n",
    "    ['maw', 'more'],\n",
    "    ['me', 'mi'],\n",
    "    ['mean', 'mien'],\n",
    "    ['meat', 'meet', 'mete'],\n",
    "    ['medal', 'meddle'],\n",
    "    ['metal', 'mettle'],\n",
    "    ['meter', 'metre'],\n",
    "    ['might', 'mite'],\n",
    "    ['miner', 'minor', 'mynah'],\n",
    "    ['mind', 'mined'],\n",
    "    ['missed', 'mist'],\n",
    "    ['moat', 'mote'],\n",
    "    ['mode', 'mowed'],\n",
    "    ['moor', 'more'],\n",
    "    ['moose', 'mousse'],\n",
    "    ['morning', 'mourning'],\n",
    "    ['muscle', 'mussel'],\n",
    "    ['naval', 'navel'],\n",
    "    ['nay', 'neigh'],\n",
    "    ['nigh', 'nye'],\n",
    "    ['none', 'nun'],\n",
    "    ['od', 'odd'],\n",
    "    ['ode', 'owed'],\n",
    "    ['oh', 'owe'],\n",
    "    ['one', 'won'],\n",
    "    ['packed', 'pact'],\n",
    "    ['packs', 'pax'],\n",
    "    ['pail', 'pale'],\n",
    "    ['pain', 'pane'],\n",
    "    ['pair', 'pare', 'pear'],\n",
    "    ['palate', 'palette', 'pallet'],\n",
    "    ['pascal', 'paschal'],\n",
    "    ['paten', 'patten', 'pattern'],\n",
    "    ['pause', 'paws', 'pores', 'pours'],\n",
    "    ['peace', 'piece'],\n",
    "    ['peak', 'peek', 'pique', 'peke'],\n",
    "    ['pea', 'pee'],\n",
    "    ['peal', 'peel'],\n",
    "    ['pearl', 'purl'],\n",
    "    ['pedal', 'peddle'],\n",
    "    ['peer', 'pier'],\n",
    "    ['pi', 'pie'],\n",
    "    ['pica', 'pika'],\n",
    "    ['place', 'plaice'],\n",
    "    ['plain', 'plane'],\n",
    "    ['pleas', 'please'],\n",
    "    ['pole', 'poll'],\n",
    "    ['plum', 'plumb'],\n",
    "    ['poof', 'pouffe'],\n",
    "    ['practice', 'practise'],\n",
    "    ['praise', 'prays', 'preys'],\n",
    "    ['principal', 'principle'],\n",
    "    ['profit', 'prophet'],\n",
    "    ['quarts', 'quartz'],\n",
    "    ['quean', 'queen'],\n",
    "    ['rain', 'reign', 'rein'],\n",
    "    ['raise', 'rays', 'raze'],\n",
    "    ['rap', 'wrap'],\n",
    "    ['raw', 'roar'],\n",
    "    ['read', 'reed'],\n",
    "    ['read', 'red'],\n",
    "    ['real', 'reel'],\n",
    "    ['reek', 'wreak'],\n",
    "    ['rest', 'wrest'],\n",
    "    ['retch', 'wretch'],\n",
    "    ['review', 'revue'],\n",
    "    ['rheum', 'room'],\n",
    "    ['right', 'rite', 'wright', 'write'],\n",
    "    ['ring', 'wring'],\n",
    "    ['road', 'rode'],\n",
    "    ['roe', 'row'],\n",
    "    ['role', 'roll'],\n",
    "    ['roo', 'roux', 'rue'],\n",
    "    ['rood', 'rude'],\n",
    "    ['root', 'route'],\n",
    "    ['rose', 'rows'],\n",
    "    ['rota', 'rotor'],\n",
    "    ['rote', 'wrote'],\n",
    "    ['rough', 'ruff'],\n",
    "    ['rouse', 'rows'],\n",
    "    ['rung', 'wrung'],\n",
    "    ['rye', 'wry'],\n",
    "    ['saver', 'savour'],\n",
    "    ['scull', 'skull'],\n",
    "    ['spade', 'spayed'],\n",
    "    ['sale', 'sail'],\n",
    "    ['sane', 'seine'],\n",
    "    ['satire', 'satyr'],\n",
    "    ['sauce', 'source'],\n",
    "    ['saw', 'soar', 'sore'],\n",
    "    ['scene', 'seen'],\n",
    "    ['sea', 'see'],\n",
    "    ['seam', 'seem'],\n",
    "    ['sear', 'seer', 'sere'],\n",
    "    ['seas', 'sees', 'seize'],\n",
    "    ['shake', 'sheikh'],\n",
    "    ['sew', 'so', 'sow'],\n",
    "    ['shear', 'sheer'],\n",
    "    ['shoe', 'shoo'],\n",
    "    ['sic', 'sick'],\n",
    "    ['side', 'sighed'],\n",
    "    ['sign', 'sine'],\n",
    "    ['sink', 'synch'],\n",
    "    ['slay', 'sleigh'],\n",
    "    ['sloe', 'slow'],\n",
    "    ['sole', 'soul'],\n",
    "    ['some', 'sum'],\n",
    "    ['son', 'sun'],\n",
    "    ['sort', 'sought'],\n",
    "    ['spa', 'spar'],\n",
    "    ['staid', 'stayed'],\n",
    "    ['stair', 'stare'],\n",
    "    ['stake', 'steak'],\n",
    "    ['stalk', 'stork'],\n",
    "    ['stationary', 'stationery'],\n",
    "    ['steal', 'steel'],\n",
    "    ['stile', 'style'],\n",
    "    ['storey', 'story'],\n",
    "    ['straight', 'strait'],\n",
    "    ['sweet', 'suite'],\n",
    "    ['swat', 'swot'],\n",
    "    ['tacks', 'tax'],\n",
    "    ['tale', 'tail'],\n",
    "    ['talk', 'torque'],\n",
    "    ['tare', 'tear'],\n",
    "    ['taught', 'taut', 'tort'],\n",
    "    ['te', 'tea', 'tee', 't', 'ti'],\n",
    "    ['team', 'teem'],\n",
    "    ['tear', 'tier'],\n",
    "    ['teas', 'tease'],\n",
    "    ['terce', 'terse'],\n",
    "    ['tern', 'turn'],\n",
    "    ['there', 'their', \"they're\"],\n",
    "    ['threw', 'through', 'thru'],\n",
    "    ['throes', 'throws'],\n",
    "    ['throne', 'thrown'],\n",
    "    ['thyme', 'time'],\n",
    "    ['tic', 'tick'],\n",
    "    ['tide', 'tied'],\n",
    "    ['tire', 'tyre'],\n",
    "    ['to', 'too', 'two'],\n",
    "    ['toad', 'toed', 'towed'],\n",
    "    ['told', 'tolled'],\n",
    "    ['tole', 'toll'],\n",
    "    ['ton', 'tun'],\n",
    "    ['tor', 'tore'],\n",
    "    ['tough', 'tuff'],\n",
    "    ['troop', 'troupe'],\n",
    "    ['tuba', 'tuber'],\n",
    "    ['vain', 'vane', 'vein'],\n",
    "    ['vale', 'veil'],\n",
    "    ['vial', 'vile'],\n",
    "    ['vice', 'vise'],\n",
    "    ['wade', 'weighed'],\n",
    "    ['weak', 'week'],\n",
    "    ['we', 'wee', 'whee'],\n",
    "    ['way', 'weigh', 'whey'],\n",
    "    ['wax', 'whacks'],\n",
    "    ['wart', 'wort'],\n",
    "    ['watt', 'what'],\n",
    "    ['warn', 'worn'],\n",
    "    ['ware', 'wear', 'where'],\n",
    "    ['war', 'wore'],\n",
    "    ['wall', 'waul'],\n",
    "    ['waive', 'wave'],\n",
    "    ['wait', 'weight'],\n",
    "    ['wail', 'wale', 'whale'],\n",
    "    ['wain', 'wane'],\n",
    "    [\"we'd\", 'weed'],\n",
    "    ['weal', \"we'll\", 'wheel'],\n",
    "    ['wean', 'ween'],\n",
    "    ['weather', 'whether'],\n",
    "    ['weaver', 'weever'],\n",
    "    ['weir', \"we're\"],\n",
    "    ['were', 'whirr'],\n",
    "    ['wet', 'whet'],\n",
    "    ['wheald', 'wheeled'],\n",
    "    ['which', 'witch'],\n",
    "    ['whig', 'wig'],\n",
    "    ['while', 'wile'],\n",
    "    ['whine', 'wine'],\n",
    "    ['whirl', 'whorl'],\n",
    "    ['whirled', 'world'],\n",
    "    ['whit', 'wit'],\n",
    "    ['white', 'wight'],\n",
    "    [\"who's\", 'whose'],\n",
    "    ['woe', 'whoa'],\n",
    "    ['wood', 'would'],\n",
    "    ['yaw', 'yore', 'your', \"you're\"],\n",
    "    ['yoke', 'yolk'],\n",
    "    [\"you'll\", 'yule']\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsan5800",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
